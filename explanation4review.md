# MochiRAG プロジェクト進捗報告とレビューのお願い

このドキュメントでは、MochiRAGプロジェクトの現在の開発進捗と、レビューをお願いしたい点について説明します。

## 1. 実装済み機能概要

現時点で実装されている主な機能は以下の通りです。

### バックエンド (FastAPI)
- **ユーザー認証**: JWT（JSON Web Token）を使用した登録・ログイン機能。ユーザー情報は `data/users.json` に保存されます。
- **ドキュメントアップロード**:
    - TXT, MD, PDF ファイルのアップロードに対応。
    - アップロードされたファイルは一時保存され、チャンク化、ベクトル化（SentenceTransformerを使用）、ChromaDBへの保存が行われます。
    - アップロードされたドキュメントのメタデータ（ファイル名、アップロード日時、チャンク数など）は `data/datasources_meta.json` に保存されます。
- **ドキュメント一覧表示**: 認証されたユーザーがアップロードしたドキュメントのメタデータ一覧を取得するAPI (`/documents/`)。
- **チャットクエリ**:
    - RAG（Retrieval Augmented Generation）パイプラインを使用したチャット応答API (`/chat/query/`)。
    - ユーザー自身のドキュメントから関連情報を検索し、Ollama (llama3モデルを想定) を使用して回答を生成するロジックが組まれています。

### コアロジック (`core/` モジュール)
- **ドキュメント処理 (`document_processor.py`)**:
    - TextLoader (TXT), UnstructuredMarkdownLoader (MD), PyPDFLoader (PDF) を使用したドキュメント読み込み。
    - RecursiveCharacterTextSplitter を用いたテキストのチャンク化。
- **ベクトル化・ベクトルストア (`vector_store.py`)**:
    - `all-MiniLM-L6-v2` モデル (SentenceTransformerEmbeddings) を使用したエンベディング生成。
    - ChromaDB を使用したベクトルデータの永続化 (`data/chroma_db` ディレクトリ)。
    - ドキュメント追加時にユーザーID、データソースIDをメタデータとして付与。
    - ユーザーIDに基づいたベクトル検索、複雑なメタデータのフィルタリング処理。
- **RAGチェーン (`rag_chain.py`)**:
    - LangChain Expression Language (LCEL) を使用したRAGパイプライン構築。
    - `ChatOllama` (llama3モデル) との連携を準備済み。
    - プロンプトテンプレート、コンテキスト整形ロジック（引用元表示のためメタデータ利用）を実装。

### フロントエンド (Streamlit)
- **ユーザー認証UI (`frontend/app.py`)**:
    - ユーザー登録フォームとログインフォーム。
    - Streamlitのセッション状態 (`st.session_state`) を使用した認証トークンとユーザー情報の管理。
- **基本UIシェル**:
    - ログイン後、サイドバーにユーザー名表示とログアウトボタンを配置。
    - 「チャット」と「ドキュメント管理」ページへのナビゲーションプレースホルダーを設置。

## 2. 次に実装予定の機能

- **フロントエンド ドキュメント管理機能**:
    - Streamlit UIを介したドキュメントアップロード機能の実装。
    - アップロード済みドキュメントの一覧表示と、可能であれば削除機能。

## 3. テストおよびレビュー手順

プロジェクトの動作確認とコードレビューのために、以下の手順と項目をご確認ください。

### 環境セットアップ
1.  **バックエンド FastAPI サーバーの起動**:
    プロジェクトのルートディレクトリで以下のコマンドを実行します。
    ```bash
    python -m uvicorn backend.main:app --reload --port 8000
    ```
2.  **Streamlit フロントエンドの起動**:
    プロジェクトのルートディレクトリで以下のコマンドを実行します。（事前に `pip install -r frontend/requirements.txt` を実行してください）
    ```bash
    streamlit run frontend/app.py
    ```
    *注意: フロントエンドとバックエンドは同時に実行されている必要があります。*

### レビュー項目
1.  **ユーザー登録**:
    - フロントエンドの登録ページから新しいユーザーを作成します。
    - `data/users.json` ファイルに新しいユーザー情報が正しく保存されていることを確認します。
2.  **ログイン・ログアウト**:
    - 登録したユーザーでフロントエンドからログインおよびログアウトができることを確認します。
    - ログイン後、サイドバーにユーザー名が表示されることを確認します。
3.  **ドキュメントアップロード (APIレベルでの確認)**:
    - PostmanやcurlなどのAPIクライアントを使用します。
    - まず、いずれかのユーザーでログインし、発行されたアクセストークンを取得します。
    - 取得したトークンをAuthorizationヘッダー (Bearer トークン) に設定し、`/documents/upload/` エンドポイントに対してTXT, MD, PDFファイルをPOSTします。
    - アップロード後、`data/chroma_db/` ディレクトリ（またはその中のファイル）が更新されること、および `data/datasources_meta.json` にアップロードしたドキュメントのメタデータがユーザーIDに紐づいて記録されていることを確認します。
4.  **ドキュメント一覧表示 (APIレベルでの確認)**:
    - 上記と同様にAPIクライアントとトークンを使用し、`/documents/` エンドポイントにGETリクエストを送信します。
    - 認証されたユーザーがアップロードしたドキュメントのメタデータ一覧が返却されることを確認します。
5.  **チャット機能 (APIレベルでの確認、限定的)**:
    - APIクライアントとトークンを使用し、`/chat/query/` エンドポイントにPOSTリクエストを送信します。リクエストボディには以下のようなJSONを含めます。
      ```json
      {
        "question": "アップロードしたドキュメントに関する質問",
        "data_source_ids": ["（特定のデータソースID、任意）"]
      }
      ```
    - **重要**: 現在の開発・テスト環境ではOllamaサーバーが実行されていないため、実際にLLMからの回答は得られません。`get_rag_response`関数内でLLM連携部分がエラーとなり、「I'm sorry, but I encountered an error...」のようなエラーメッセージが返却されることを期待します。これは、RAGチェーンが呼び出されているものの、LLMとの接続に失敗していることを示します。
6.  **コードレビュー**:
    - 主要なロジックが実装されている以下のファイルのレビューをお願いします。
        - `backend/main.py` (APIエンドポイント、ドキュメント処理フロー)
        - `core/document_processor.py` (ファイル読み込み、チャンキング)
        - `core/vector_store.py` (エンベディング、ChromaDB連携)
        - `core/rag_chain.py` (RAGパイプライン、プロンプト)
        - `frontend/app.py` (Streamlit UI、認証フロー)

### 注意事項
- 完全なチャット機能のテストは、Ollamaが利用可能な環境で行う必要があります。現在のレビュー段階では、APIエンドポイントがRAGチェーンを呼び出し、LLM接続部分で想定通りのエラーハンドリング（またはフォールバックメッセージ）が機能するかを確認することが主眼となります。
- APIの動作、データ（ユーザー情報、ドキュメントメタデータ、ベクトルデータ）の永続化、フロントエンドの認証フローと基本的な画面遷移を中心にレビューいただけると幸いです。

お忙しいところ恐縮ですが、ご確認のほどよろしくお願いいたします。
