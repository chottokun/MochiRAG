# MochiRAG 実装設計書

## 1. 概要

本ドキュメントは、MochiRAGのコアコンポーネントである `LLMManager` と `RetrieverManager` の内部実装、およびそれらが提供する各種戦略について詳細に解説します。

## 2. LLMプロバイダーの管理 (`LLMManager`)

`core/llm_manager.py` に実装されている `LLMManager` は、複数の大規模言語モデル（LLM）プロバイダーを効率的に管理するためのシングルトンクラスです。

### 2.1. 役割と設計

- **シングルトンパターン**: アプリケーション全体で唯一のインスタンスを保証し、LLMクライアントの生成を一度だけに限定することで、リソースを効率的に使用します。
- **動的インスタンス化**: `config/strategies.yaml` の `llms` セクションの設定に基づき、要求されたLLMプロバイダー (`Ollama`, `OpenAI`, `Azure OpenAI`, `Gemini`) に対応するLangChainの `Chat` モデルを動的にインスタンス化します。
- **キャッシュ**: 一度生成されたLLMクライアントは内部でキャッシュされ、後続の呼び出しでは再利用されます。
- **環境変数による設定上書き**: `.env` ファイルに `LLM_PROVIDER` と `LLM_MODEL_NAME` を設定することで、`strategies.yaml` の設定を上書きし、デフォルトで使用するLLMを簡単に切り替えることができます。

### 2.2. 設定例

`config/strategies.yaml` で以下のようにLLMを定義します。

```yaml
llms:
  # Ollama
  gemma3:4b-it-qat:
    provider: ollama
    model_name: "gemma3:4b-it-qat"
    base_url: "http://localhost:11434"

  # OpenAI
  gpt-4o:
    provider: openai
    model_name: "gpt-4o"
    api_key: "YOUR_OPENAI_API_KEY"

  # Azure OpenAI
  azure-gpt-4:
    provider: azure
    deployment_name: "YOUR_DEPLOYMENT_NAME"
    azure_endpoint: "YOUR_AZURE_ENDPOINT"
    api_version: "2024-02-01"
    api_key: "YOUR_AZURE_API_KEY"
```

## 3. ドキュメントの取り込みと解析 (`IngestionService`)

`core/ingestion_service.py` に実装されている `IngestionService` は、アップロードされたファイルの解析、テキストの分割（チャンキング）、そしてベクトルストアへの登録を担当します。

### 3.1. 高度なPDF解析

MochiRAGは、標準的なPDFパーサーの代わりに `DoclingLoader` (`langchain-docling`) を採用しています。これにより、以下のような高度な機能が実現されます。

- **レイアウト認識**: 段落、リスト、ヘッダー・フッターなどの文書構造を維持したままテキストを抽出します。
- **テーブル解析**: PDF内のテーブルをMarkdown形式のテーブルとして正確に抽出します。
- **OCR処理**: スキャンされたドキュメントや画像に含まれる文字を自動的に認識し、テキスト化します。

このアプローチにより、文書の構造情報が失われにくくなり、RAGの検索精度と回答品質の向上に貢献します。

### 3.2. チャンク設定のカスタマイズ

ドキュメントをベクトル化する際のチャンクサイズとオーバーラップは、RAGの性能に大きな影響を与えます。MochiRAGでは、これらの値を `.env` ファイルで簡単に設定できます。

- **`CHUNK_SIZE`**: 各チャンクのおおよその文字数。
- **`CHUNK_OVERLAP`**: 隣接するチャンク間で重複させる文字数。

これらの環境変数を設定すると、`config/strategies.yaml` のデフォルト値が上書きされ、実験やチューニングが容易になります。

---

## 4. RAG検索戦略の管理 (`RetrieverManager`)

`core/retriever_manager.py` に実装されている `RetrieverManager` は、`config/strategies.yaml` の設定に基づき、様々なRAG検索戦略を動的に構築・提供します。以下に、現在実装されている各戦略の詳細を解説します。

---

### 4.1. `BasicRetrieverStrategy` (基本戦略)

- **概要**: 最も基本的なベクトル検索戦略。ユーザーが選択した複数のデータソース（個人用・共有）を横断して同時に検索します。
- **動作原理**:
    1.  選択されたデータセットIDに基づき、個人用 (`user_{user_id}`) と共有用 (`shared_{db_name}`) の各ChromaDBコレクションに対応するリトリーバーを生成します。
    2.  生成されたリトリーバーが複数ある場合、LangChainの `EnsembleRetriever` を用いてそれらを束ねます。
    3.  `EnsembleRetriever` は、各リトリーバーからの検索結果をReciprocal Rank Fusion (RRF) アルゴリズムで統合し、最も関連性が高いと判断されたドキュメントを最終的な検索結果として返します。
- **長所**: 高速かつシンプル。複数のナレッジベースを一度に検索できるため利便性が高いです。
- **短所**: 質問文をそのままベクトル化するため、単語の表面的な意味に頼りがちで、複雑な意図を汲み取った検索は困難です。

---

### 4.2. `MultiQueryRetrieverStrategy` (複数クエリ生成戦略)

- **概要**: ユーザーの質問をLLMが分析し、異なる角度からの複数の検索クエリを自動生成して検索を実行する戦略です。
- **動作原理**:
    1.  `BasicRetrieverStrategy` で構築されたリトリーバーを内部に持ちます。
    2.  ユーザーの質問を受け取ると、LLMがその質問を解釈し、類似の質問や異なる視点からの質問を3〜5個生成します (例: 「RAGとは？」→「Retrieval-Augmented Generationの仕組みは？」、「RAGの利点は？」)。
    3.  生成されたすべてのクエリで並行して検索を実行し、得られた結果を統合して返します。
- **長所**: 検索網羅性が向上し、ユーザーが曖昧な質問をしても関連ドキュメントを見つけやすくなります。
- **短所**: LLMの呼び出しが増えるため、応答速度がやや低下し、コストが増加します。

---

### 4.3. `ContextualCompressionRetrieverStrategy` (コンテキスト圧縮戦略)

- **概要**: 一旦取得したドキュメントの内容をLLMが要約・抽出し、質問に直接関連する部分だけを最終的なコンテキストとして利用する戦略です。
- **動作原理**:
    1.  `BasicRetrieverStrategy` でドキュメントを検索します。
    2.  取得した各ドキュメントの内容とユーザーの質問をLLMに渡し、「この質問に答えるために必要な情報だけを抽出せよ」と指示します。
    3.  LLMによって圧縮・抽出された情報のみを後段のRAGプロンプトに渡します。
- **長所**: プロンプトに含めるコンテキストのノイズが減少し、LLMがより正確な回答を生成しやすくなります。
- **短所**: 検索後に再度LLMを呼び出すため、応答速度が低下し、コストが増加します。

---

### 4.4. `ParentDocumentRetrieverStrategy` (親子ドキュメント戦略)

- **概要**: ドキュメントを大きな「親チャンク」と、検索対象となる小さな「子チャンク」に分割して管理する戦略です。検索は子チャンクで行い、回答生成には親チャンクの完全なコンテキストを利用します。
- **動作原理**:
    1.  **Ingestion時**: ドキュメントを例えば400文字の「子チャンク」と、2000文字の「親チャンク」に分割します。子チャンクのベクトルのみをVector DBに保存し、親チャンクの全文はSQLデータベース (`SQLDocStore`) に保存します。
    2.  **検索時**: ユーザーの質問で子チャンクを検索します。
    3.  **回答生成時**: 見つかった子チャンクが属する親チャンクの全文をSQLデータベースから取得し、LLMに渡します。
- **長所**: 検索の精度（小さいチャンク）と、回答生成時の文脈の豊富さ（大きいチャンク）を両立できます。
- **短所**: Ingestion時の処理が複雑になり、SQLデータベースという追加の依存コンポーネントが必要になります。

---

### 4.5. `StepBackPromptingRetrieverStrategy` (一歩下がった質問戦略)

- **概要**: ユーザーの具体的な質問から一歩引いた、より一般的・抽象的な質問をLLMに生成させ、その質問で検索を行う戦略です。
- **動作原理**:
    1.  ユーザーの質問 (例: 「MochiRAGの`BasicRetrieverStrategy`はEnsembleRetrieverを使っていますか？」) を受け取ります。
    2.  LLMがその質問の背景にある、より一般的な概念についての質問を生成します (例: 「RAGにおけるEnsembleRetrieverの役割とは？」)。
    3.  この生成された一般的な質問を使って `BasicRetrieverStrategy` で検索を実行します。
- **長所**: 元の質問が具体的すぎたり、ニッチすぎて直接的な回答が見つからない場合に、関連性の高い上位概念のドキュメントを検索できる可能性が高まります。
- **短所**: LLMによる質問生成の品質に依存し、意図しない検索結果になる可能性もあります。

---

### 4.6. `HyDE` (Hypothetical Document Embeddings) Strategy

**注意**: HyDE戦略に関連する実装は、LangChainのバージョン間の互換性問題により、現在コードベースから**削除されています**。

HyDEは、質問に対する「架空の回答」をまずLLMに生成させ、その架空の回答をベクトル化して検索に利用する高度な戦略です。将来的に再導入を検討する際は、LangChainのバージョン互換性を慎重に検証し、十分なテストを行う必要があります。

---

### 4.7. `ACERetrieverStrategy` (自己進化型コンテキスト戦略)

- **概要**: Agentic Context Engineering (ACE) のコンセプトに基づき、システムがユーザーとの対話から学び、知識を継続的に進化させていく自己改善型の検索戦略です。過去の対話から生成された「進化したコンテキスト」を、将来の検索に活用します。本戦略を含む、プロンプト駆動型RAG戦略のカスタマイズ方法については、[こちらのガイド](./../guides/customizing_prompts.md)を参照してください。
- **動作原理**:
    1.  **コンテキスト検索**: ユーザーの質問を受け取ると、まずLLMが質問の主要な「トピック」を抽出します。このトピックをキーとして、SQLデータベースに格納されている `EvolvedContext` テーブルを検索し、過去の対話から生成・蓄積された関連性の高い知識や戦略を取得します。
    2.  **基本検索の実行**: 同時に、`BasicRetrieverStrategy` を用いて、ベクトルストアから関連ドキュメントを検索します。
    3.  **結果の統合**: データベースから取得した「進化したコンテキスト」と、ベクトルストアから取得したドキュメントを統合します。進化したコンテキストは、より洗練された知識である可能性が高いため、プロンプトの先頭に配置され、優先的にLLMに渡されます。
    4.  **自己進化 (バックグラウンド処理)**: ユーザーに応答が返された後、FastAPIのバックグラウンドタスクとして自己進化プロセスが非同期で実行されます。
        -   **知識の抽出**: 今回のユーザーの質問と最終的な回答をLLMに渡し、「この対話から得られた、将来役立つ普遍的な知見は何か？」を分析させ、新しい「進化したコンテキスト」を生成します。
        -   **データベースへの保存**: 生成された新しいコンテキストを、質問から抽出したトピックと共に `EvolvedContext` テーブルに保存します。
- **長所**:
    - **自己改善**: システムを使えば使うほど、対話を通じて得られた知見がデータベースに蓄積され、検索精度と回答の質が継続的に向上します。
    - **知識の再利用**: 特定のトピックに関する優れた回答や補足情報が、将来の同様の質問に対して自動的に再利用されるようになります。
    - **非同期進化**: コンテキストの進化プロセスはバックグラウンドで実行されるため、ユーザーの応答時間に影響を与えません。
- **短所**:
    - **ストレージコストの増加**: 対話ごとに新しいコンテキストが生成されるため、SQLデータベースの容量が増加します。
    - **コンテキスト品質の依存**: 進化するコンテキストの品質は、知識を抽出するLLMの性能に大きく依存します。
