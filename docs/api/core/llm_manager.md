# API: `llm_manager.py`

このモジュールは、アプリケーション全体で利用される大規模言語モデル（LLM）のクライアントインスタンスを一元的に管理する責務を持ちます。

## `LLMManager` クラス

`LLMManager` は、シングルトンパターンで実装されており、LLMクライアントの効率的な再利用と、設定に基づいた動的な生成を実現します。

### 設計思想

- **シングルトン (Singleton)**: アプリケーション内で `LLMManager` のインスタンスが常に一つであることを保証します。これにより、LLMクライアントが不必要に複数生成されるのを防ぎ、リソースを節約します。
- **ファクトリ (Factory)**: `config/strategies.yaml` の設定に基づき、`Ollama`, `OpenAI`, `Azure OpenAI`, `Gemini` といった様々なプロバイダーに対応する適切なLangChainの `Chat` モデルのインスタンスを生成します。
- **キャッシュ (Cache)**: 一度生成されたLLMクライアントのインスタンスは、モデル名をキーとして内部の辞書にキャッシュされます。以降、同じモデルへのリクエストがあった場合は、新しいインスタンスを生成する代わりにキャッシュされたオブジェクトを即座に返します。

### `get_llm(self, role: Optional[str] = None) -> BaseChatModel`

設定されたロール（役割）に基づいて、LLMクライアントのインスタンスを取得します。

- **パラメータ:**
  - `role` (Optional[str]): `config/strategies.yaml` の `llms` セクションで定義されたキー名（例: `gpt-4o`, `gemma3:4b-it-qat`）。このロールに基づいて使用するLLMが決まります。`None` の場合は、設定ファイルで指定されたデフォルトのLLMが使用されます。

- **戻り値:**
  - `langchain_core.language_models.BaseChatModel`: LangChainのチャットモデルインターフェースに準拠したLLMクライアントのインスタンス。

- **処理の概要:**
  1.  指定された `role` に基づいて、`config_manager` からLLMの設定情報を取得します。
  2.  設定されたモデル名が内部キャッシュに存在するか確認します。
  3.  キャッシュに存在すれば、そのインスタンスを返します。
  4.  キャッシュに存在しない場合は、設定情報の `provider` フィールド（`ollama`, `openai`など）を元に、適切な `Chat` モデル（`ChatOllama`, `ChatOpenAI`など）を動的にインスタンス化します。
  5.  新しく生成したインスタンスをキャッシュに格納し、それを返します。
  6.  サポートされていないプロバイダーが指定された場合は `ValueError` を送出します。
