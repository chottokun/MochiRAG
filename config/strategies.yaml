# Configuration for MochiRAG models and strategies

# Vector Store Configuration
# Configure the vector store used by the application.
# `provider`: Currently only 'chromadb' is supported.
# `mode`: 'persistent' for local file-based storage, 'http' for client-server mode.
# `host`: The hostname of the ChromaDB server (only for 'http' mode).
# `port`: The port of the ChromaDB server (only for 'http' mode).
# `path`: The local directory to store database files (only for 'persistent' mode).
vector_store:
  provider: chromadb
  mode: http # 'persistent # Can be 'persistent' or 'http'
  host: localhost
  port: 8001  # Changed to 8001 to avoid conflict with backend uvicorn (8000)
  path: "chroma_db"

# Embedding models configuration
embeddings:
  # Default model for embedding text. Excellent performance for its size.
  all-MiniLM-L6-v2:
    provider: ollama # Indicates it's a SentenceTransformer model
    model_name: "mahonzhan/all-MiniLM-L6-v2:latest"
    base_url: "http://localhost:11434"

  ruri-v3-30m:
    provider: openai_compatible
    model_name: "cl-nagoya/ruri-v3-30m"
    base_url: "http://embedding_api:8000/v1"

# Large Language Models (LLM) configuration
llms:
  # Role mappings: Define which provider configuration to use for a given role.
  roles:
    main: gemma3:4b-it-qat
    fast: gemma3:4b-it-qat
    # vision: gemini-1.5-pro # Example for future extension

  # Provider configurations: Define the settings for each individual model.
  providers:
    gemma3:4b-it-qat:
      provider: ollama  
      model_name: "gemma3:4b-it-qat"
      base_url: "http://localhost:11434"

    gpt-4o:
      provider: openai
      model_name: "gpt-4o"
      api_key: "${OPENAI_API_KEY}" # Loaded from .env file
      temperature: 0.7

    azure-gpt-4:
      provider: azure
      deployment_name: "YOUR_DEPLOYMENT_NAME"
      azure_endpoint: "YOUR_AZURE_ENDPOINT"
      api_version: "2024-02-01"
      api_key: "${AZURE_OPENAI_API_KEY}" # Loaded from .env file
      temperature: 0.7

    gemini-1.5-pro:
      provider: gemini
      model_name: "gemini-1.5-pro-latest"
      api_key: "${GOOGLE_API_KEY}" # Loaded from .env file
      temperature: 0.7

# Retriever strategies configuration
retrievers:
  # Configuration for the basic semantic search
  basic:
    strategy_class: "BasicRetrieverStrategy"
    description: "A standard vector similarity search."
    parameters:
      k: 5 # Number of documents to retrieve
      # Text splitting (chunking) controls used when ingesting documents for this retriever.
      # You can tune these to balance retrieval granularity vs. context size.
      # - chunk_size: approx. characters per chunk (default shown here)
      # - chunk_overlap: characters overlap between adjacent chunks
      # Example (defaults):
      chunk_size: 1000
      chunk_overlap: 200
  multiquery:
    strategy_class: "MultiQueryRetrieverStrategy"
    description: "Generates multiple queries from different perspectives for a given user question."
    parameters: {}
  compression:
    strategy_class: "ContextualCompressionRetrieverStrategy"
    description: "Compresses retrieved documents to extract only the relevant information."
    parameters: {}
  parent_document:
    strategy_class: "ParentDocumentRetrieverStrategy"
    description: "Retrieves smaller chunks of documents and returns the parent document."
    parameters:
      # Parent/child chunking controls. Parent chunks are larger documents
      # that will be returned to the LLM, while child chunks are the
      # smaller indexed chunks used for retrieval.
      parent_chunk_size: 2000
      parent_chunk_overlap: 200
      child_chunk_size: 400
      child_chunk_overlap: 100
  step_back:
    strategy_class: "StepBackPromptingRetrieverStrategy"
    description: "Generates a more general 'step-back' question to improve retrieval for broad queries."
    parameters: {}
  deeprag:
    strategy_class: "DeepRAGStrategy"
    description: "Decomposes complex questions into a series of simpler subqueries to build a comprehensive answer."
    parameters: {}
  ace:
    strategy_class: "ACERetrieverStrategy"
    description: "A self-evolving strategy that learns from interactions to improve context retrieval over time."
    parameters: {}

# --------------------------------------------------------------------------
# Prompt Templates
# --------------------------------------------------------------------------
# This section defines the prompt templates used by various RAG strategies.
# You can customize these prompts to tailor the behavior of the LLM.
prompts:
  step_back: |
    You are an expert at world knowledge. I am going to ask you a question. Your job is to formulate a single, more general question that captures the essence of the original question. Frame the question from the perspective of a historian or a researcher.
    Original question: {question}
    Step-back question:
  ace_topic: |
    Based on the following user question, identify the main topic or entity in one or two words.
    Your answer should be concise and suitable for use as a database search key.
    Examples:
    - Question: "How does the ParentDocumentRetriever work in MochiRAG?" -> Answer: "ParentDocumentRetriever"
    - Question: "Tell me about ensemble retrievers" -> Answer: "EnsembleRetriever"
    - Question: "What are the key features?" -> Answer: "Features"

    Original question: {question}
    Topic:
  ace_evolution: |
    You are an expert in synthesizing knowledge. Based on the user's question and the provided answer, formulate a single, concise, and reusable insight. This insight should be a piece of general knowledge that could help answer similar questions more effectively in the future.

    Do not repeat the question or the answer. Focus on extracting the core principle or strategy.

    User Question:
    "{question}"

    Provided Answer:
    "{answer}"

    Concise Insight: