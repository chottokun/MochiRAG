embedding_strategies:
  default: sentence_transformer_all-MiniLM-L6-v2
  available:
    - name: sentence_transformer_all-MiniLM-L6-v2
      type: sentence_transformer
      model_name: "all-MiniLM-L6-v2"
    # - name: ollama_nomic-embed-text  # Ollamaを利用する場合のエンベディング戦略例
    #   type: ollama_embedding
    #   model_name: "nomic-embed-text"
    #   # base_url: "http://localhost:11434" # 必要に応じてOllamaのURLを指定

chunking_strategies:
  default: recursive_cs1000_co200
  available:
    - name: recursive_cs1000_co200
      type: recursive_text_splitter
      params:
        chunk_size: 1000
        chunk_overlap: 200
    - name: recursive_cs500_co50
      type: recursive_text_splitter
      params:
        chunk_size: 500
        chunk_overlap: 50
    # - name: semantic_chunker_percentile # SemanticChunkerを利用する場合の例
    #   type: semantic_chunker
    #   # SemanticChunkerはエンベディングモデルに依存するため、どのエンベディング戦略を使用するか参照させる必要がある
    #   embedding_strategy_ref: sentence_transformer_all-MiniLM-L6-v2
    #   params:
    #     breakpoint_threshold_type: "percentile" # "standard_deviation", "interquartile" なども指定可能

rag_search_strategies:
  default: basic
  available:
    - name: basic
      type: basic # <--- Add type
      description: "Standard vector similarity search."
      # basic戦略に特有のデフォルトパラメータがあればここに記述 (例: n_results)
      # default_n_results: 3
    - name: multi_query
      type: multi_query # <--- Add type
      description: "Generates multiple queries from the original to broaden search."
      # default_n_results: 3
    - name: contextual_compression
      type: contextual_compression # <--- Add type
      description: "Retrieves, then compresses documents to only relevant parts."
      # default_n_results: 5 # Compressorのために初期取得数を多めに設定することが多い
    - name: parent_document
      type: parent_document # <--- Add type
      description: "Retrieves child chunks and returns parent documents. (Currently falls back to basic)"
      # default_n_results: 3
    - name: deep_rag
      type: deep_rag # <--- Add type
      description: "Decomposes complex queries into sub-queries for staged retrieval."
      # default_n_results_per_subquery: 3 # サブクエリごとの取得数
      # default_max_sub_queries: 3       # 最大サブクエリ数
      # query_decomposition_prompt_id: "deep_rag_decompose_default" # プロンプトテンプレートID（将来的に管理する場合）

# (オプション) プロンプトテンプレートの管理セクション
# prompt_templates:
#   - id: deep_rag_decompose_default
#     template: |
#       You are an expert at query decomposition... (以下略、core/retriever_manager.py内のテンプレート)

llm_config:
  default_provider: ollama_chat # LLMプロバイダーのデフォルトを指定
  providers:
    - name: ollama_chat
      type: ollama # core/llm_manager.py (新規作成想定) でこのtypeを解釈
      model: "gemma3:4b-it-qat" # core/rag_chain.py で使用していたモデル
      temperature: 0
      # base_url: "http://localhost:11434" # OllamaのデフォルトURLだが明示も可能
    # - name: openai_gpt35_turbo
    #   type: openai
    #   model: "gpt-3.5-turbo"
    #   api_key_env: "OPENAI_API_KEY" # 環境変数からAPIキーを読み込む場合
    #   temperature: 0.1
