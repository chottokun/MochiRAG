# MochiRAG

MochiRAG is a multi-tenant, Retrieval-Augmented Generation (RAG) application that allows users to upload their own documents and interact with an AI to get answers based on the provided content.

This application features a Python backend built with FastAPI and a reactive frontend built with Streamlit.

## âœ¨ Features

- **Secure Multi-Tenancy**: User data is completely isolated. A user can only access the documents and datasets they own.
- **User Authentication**: Secure sign-up and login functionality.
- **Dataset Management**: Create and delete datasets to organize documents.
- **Document Management**: Upload documents (`.txt`, `.md`, `.pdf`) to specific datasets and delete them.
- **RAG Chat Interface**:
    - Ask questions in natural language.
    - Select one or more datasets to query against.
    - Choose from various RAG strategies for retrieval (see "RAG Strategies" section below).
    - View the sources used by the LLM to generate an answer.

### Shared Databases (Admin-Managed)

In addition to personal datasets, administrators can create shared databases that are accessible to all users. This is useful for providing common, read-only knowledge bases.

Shared databases are created using the `cli.py` script. The script ingests all documents from a specified source directory into a new vector collection.

**To create a shared database:**

1.  Place all your source documents (e.g., `.pdf`, `.txt`, `.md`) into a directory.
2.  Run the following command from the project root, ensuring your virtual environment is active:

    ```bash
    # Set environment variables to connect to running services
    export CHROMA_HOST=localhost
    export CHROMA_PORT=8001 # Use the host port mapped in docker-compose
    export OLLAMA_BASE_URL=http://localhost:11434 # If Ollama runs on the host

    python cli.py create-shared-db --name "My Shared DB" --source-dir /path/to/your/docs
    ```

3.  The new shared database will automatically appear in the "Select a Dataset" dropdown for all users in the chat interface.

The configuration for these shared databases is stored in `shared_dbs.json`, which is generated by the script. It is recommended to add this file to your `.gitignore`.

### RAG Strategies

MochiRAG supports multiple RAG (Retrieval-Augmented Generation) strategies, allowing users to experiment with different retrieval approaches. You can select your preferred strategy in the chat interface settings.

- **Basic (Vector Search)**: A standard semantic search based on vector similarity. Documents are split into chunks, embedded, and stored in a vector database. Retrieval involves finding the `k` most similar chunks to the query.
- **Multi-Query Retriever**: Generates multiple variations of the user's question to retrieve a broader set of relevant documents, helping to overcome the limitations of single-query similarity search.
- **Contextual Compression Retriever**: Retrieves a larger set of documents and then uses an LLM to compress and extract only the most relevant information from those documents, reducing noise.
- **Parent Document Retriever**: Stores smaller, highly relevant "child" chunks in the vector database, but retrieves and provides the larger "parent" document to the LLM for context. This helps maintain context while still leveraging precise retrieval.
- **DeepRAG**: A multi-step reasoning strategy that decomposes complex questions into simpler subqueries. It iteratively retrieves information for each subquery and synthesizes the intermediate answers to form a comprehensive final response. The reasoning trace is visible in the UI.

## ðŸš€ Getting Started

### Prerequisites

- Python 3.10+
- [Poetry](https://python-poetry.org/) for dependency management (recommended).
- [uv](https://astral.sh/uv) as an alternative dependency installer.
- An Ollama instance running with a model (e.g., `gemma3:4b-it-qat`). For installation and usage, refer to the [Ollama documentation](https://ollama.com/). You can pull a model using `ollama pull gemma3:4b-it-qat`. The LLM can be configured in `config/strategies.yaml`.

### 1. Setup

Clone the repository and install the required dependencies.

```bash
git clone <repository-url>
cd MochiRAG
```

**Using Poetry (Recommended):**

```bash
poetry install
```

**Using uv (Alternative):**

First, install `uv` if you haven't already:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Then, install the project dependencies:

```bash
uv pip install -e .
```

**Note on Dependencies:** The project dependencies, especially PyTorch and CUDA packages, require a significant amount of disk space (>10 GB). Please ensure you have sufficient space before installation.

### 2. Running the Application

The application consists of two main components: the backend server and the frontend UI. You need to run both in separate terminal sessions.

**Running the Backend:**

The backend is a FastAPI application. Run it using `uvicorn`.

```bash
uvicorn backend.main:app --reload --port 8000
```
The backend server will be available at `http://localhost:8000`.

**Running the Frontend:**

The frontend is a Streamlit application.

```bash
streamlit run frontend/app.py
```
The frontend will be available at `http://localhost:8501`. Open this URL in your browser to use the application.

### Using ChromaDB in Client-Server Mode

By default, MochiRAG runs ChromaDB in a local, file-based mode. For multi-user or larger-scale deployments, you can run ChromaDB as a separate server and configure MochiRAG to connect to it.

**1. Run ChromaDB Server (using Docker):**

The easiest way to run a ChromaDB server is with Docker.

```bash
# Map host port 8001 to the Chroma container's internal port 8000 to avoid colliding with the backend (uvicorn) on 8000
docker run -p 8001:8000 chromadb/chroma
```

This will start a ChromaDB server listening on `http://localhost:8001` (host -> container 8001:8000).

**2. Configure MochiRAG:**

Next, update the `config/strategies.yaml` file to tell MochiRAG to connect to the Chroma server instead of running its own local instance.

```yaml
# config/strategies.yaml

vector_store:
  provider: chromadb
  mode: http       # Change 'persistent' to 'http'
  host: localhost  # Or the IP of your Docker host
  port: 8001
  # The 'path' property is ignored in http mode
```

After making this change, restart the MochiRAG backend. It will now connect to the external ChromaDB server.

## ðŸ§ª Running Tests

The test suite is built with `pytest` and is designed to run without any external service dependencies (it uses an in-memory SQLite database and mocks for API tests).

To run the tests, execute the following command from the root of the project directory:

```bash
pytest
```

This will discover and run all tests in the `tests/` directory.

## ðŸ“‚ Project Structure

- `backend/`: FastAPI application for API endpoints, authentication, and data management.
- `core/`: Core logic for RAG functionalities, including LLM interaction, embedding, retrieval, and vector store management.
- `frontend/`: Streamlit application for the user interface.
- `config/`: Configuration files, such as RAG strategies.
- `tests/`: Unit and integration tests for the backend and core modules.
- `docs/`: Project documentation, requirements, and design documents.

## ðŸš¢ Running with Docker Compose

You can run Chroma and the backend together with docker-compose. This compose file maps the host ports so they don't collide:

- Chroma (container port 8000) -> host port 8001
- Backend (uvicorn, container port 8000) -> host port 8000

1. Build and start services:

```bash
docker compose up --build -d
```

2. Check services:

```bash
docker compose ps
```

3. Open the services:

- Frontend (Streamlit): http://localhost:8501 (run locally or containerize separately)
- Backend API: http://localhost:8000
- Chroma HTTP API: http://localhost:8001

Notes:
- The backend container connects to Chroma using the internal Docker network; it reaches Chroma at `http://chroma:8000` (service name `chroma`).
- If you run the frontend separately on your host, ensure it talks to the backend at `http://localhost:8000`.

Ollama (external) notes:
- If you run Ollama on your host machine (not as a container in the same compose network), the backend container cannot use `localhost` to reach it. In that case either:
  - Start Ollama as a container in the same compose file (recommended), or
  - Use the Docker host gateway IP from inside the container (commonly `172.17.0.1`) and set the environment variable `OLLAMA_BASE_URL` for the `backend` service in `docker-compose.yml`, e.g. `OLLAMA_BASE_URL=http://172.17.0.1:11434`.
  - We added support for `OLLAMA_BASE_URL` in `core/config_manager.py` which will override embedding/LLM base_url entries from `config/strategies.yaml`.

  Chunking configuration (ingestion)
  ---------------------------------

  You can control how documents are split into chunks via `config/strategies.yaml` under the `retrievers` section. Two primary retrievers expose chunking options:

  - `retrievers.basic.parameters`:
    - `chunk_size` (int): approximate characters per chunk (default: 1000)
    - `chunk_overlap` (int): overlap in characters between chunks (default: 200)

  - `retrievers.parent_document.parameters`:
    - `parent_chunk_size` / `parent_chunk_overlap` (int): size/overlap for parent document chunks (defaults: 2000 / 200)
    - `child_chunk_size` / `child_chunk_overlap` (int): size/overlap for indexed child chunks (defaults: 400 / 100)

  Edit these values and restart the backend to change ingestion behavior. Reasonable defaults are included in the example `config/strategies.yaml` file.
