# MochiRAG: 認証・ユーザー管理およびRAG機能 設計要件
ただし、EntraIDを使ったユーザー認証部は実際に実装せず、簡易的なユーザ管理にとどめます。

## 1. システム概要

MochiRAGでは、ユーザーが自分のドキュメントをアップロードし、それに基づいてAIに質問できる**Retrieval-Augmented Generation (RAG)** 機能を提供します。Microsoft Entra ID (旧Azure AD)による認証・ユーザー管理機能を導入し、複数ユーザーが共存できる本番環境を想定します。各ユーザーのドキュメントは個別に管理し、検索や回答生成時に他ユーザーの情報と混在しないようデータ分離を徹底します。アプリはエンタープライズ向けとし、ドキュメントの信頼性と回答の正確性を重視して設計します。

## 2. 認証・ユーザー管理 (Microsoft Entra ID)

* **Azure AD アプリ登録**: クラウドポータルでアプリケーションを登録し、クライアントIDやリダイレクトURIを取得します。MSALライブラリ（MSAL.jsやMSAL.NETなど）を用いてOpenID Connect/OAuth2.0フローを実装し、ユーザーログイン機能を組み込みます。
* **シングルサインオン (SSO) とアクセス制御**: Microsoft Entra IDはSSOやOAuth認可、ロールベースアクセス制御(RBAC)を提供し、企業環境との統合を容易にします。必要に応じてAzure ADグループやアプリロールを定義し、管理者と一般ユーザーの権限区別を行います。
* **トークン検証**: フロントエンドで取得したIDトークン/JWTをバックエンドで検証し、トークン内のユーザーオブジェクトID (またはUPN) をキーにしてユーザーを識別します。これにより各APIリクエストでユーザー権限が確実に確認されます。
* **ユーザー情報管理**: ユーザー情報（氏名、メールアドレス等）はAzure ADが管理するため、アプリ側では最小限のストレージで済みます。必要に応じてアプリケーション用にユーザープロファイルをDBに保持し、ドキュメント/索引との紐付けを行います。外部ユーザー向けの機能要件があれば、Azure AD B2Cの採用も検討します。

## 3. 複数ユーザー対応 (マルチテナンシー)

* **データ分離**: 各ユーザー（テナント）は独自のドキュメント集合を持ち、それ以外のユーザーのデータにアクセスしないようにします。各ユーザーのデータは分離されており、他テナントの情報は参照できません。
  *Milvusベクターストアでの多テナント構成例（Database/Collection/Partition）。例えば、Collection C内にユーザー別パーティションを割り当て、クエリ時に該当ユーザーのパーティションのみを検索するように設計します。*
  このように、ベクトルストアではデータベース/コレクション/パーティションキー階層でユーザーデータを分離できます。LlamaIndexの例でも、文書メタデータにユーザーIDを付与し、検索時にフィルタリングすることでユーザーごとのデータ境界を確保しています。
* **テナント共有データ**: システム管理者向けや全ユーザー向けの共通ドキュメント（マニュアルやFAQなど）があれば、別テーブルや共通領域で管理し、検索時に全ユーザーに参照可能とします。しかし一般の質問応答ではユーザー専用データを使用する設計とします。
* **アクセスポリシー**: Azure AD認証に基づき、ユーザーは自分のドキュメントの読み書き・検索のみ許可されます。管理者ロールではシステム全体の設定変更や他ユーザーへの共有設定が可能になります。

## 3.1 ユーザ毎に複数のデータベースを持つことができます。

* ユーザーは複数のデータベースを持つことができます。各データベースには名前と説明を付与することができ。切り替えて利用することができます。

## 4. RAG機能設計

RAG(検索強化生成)では、LLMが事前定義された信頼性の高い情報源から関連情報を検索し、その情報を根拠に回答を生成します。これにより生成AIの回答は正確性が向上し、各回答には出典が付与されます。本システムでは各ユーザー固有のドキュメントを外部知識ベースとし、OllamaやAzure OpenAI等のLLMと連携して回答と引用出力を実現します。

MochiRAGでは、ユーザーがチャット時に複数のRAG戦略を選択できる機能を提供します。これにより、質問の内容や性質に応じて最適な検索・回答生成アプローチを試すことができます。

**選択可能なRAG戦略:**

*   **`basic` (基本戦略)**:
    *   ユーザーの質問をベクトル化し、ベクトルデータベース内で類似するドキュメントチャンクを検索します。
    *   取得したチャンクをコンテキストとしてLLMに渡し、回答を生成します。
    *   最も標準的なRAGアプローチです。
*   **`parent_document` (親ドキュメント参照戦略)**:
    *   基本的なベクトル検索で関連性の高い小さなチャンクを見つけ出します。
    *   その後、それらのチャンクが含まれる元の大きなドキュメント（親ドキュメント）全体、またはより大きなチャンクを参照し、コンテキストとしてLLMに渡します。
    *   より広範な文脈をLLMに提供することで、回答の質を向上させることを目指します。
    *   *注: 現在のMochiRAG実装では、この戦略は `basic` 戦略と同様の動作にフォールバックします。完全な機能のためにはドキュメントストアの統合と専用のインジェストパイプラインの改修が必要です。*
*   **`multi_query` (マルチクエリ戦略)**:
    *   ユーザーの元の質問から、LLMを使って複数の異なる視点を持つ質問（バリエーションクエリ）を生成します。
    *   これらの各バリエーションクエリでベクトル検索を実行し、得られた結果を統合します。
    *   多様な検索結果を得ることで、より網羅的な情報をLLMに提供し、回答の精度向上を目指します。
*   **`contextual_compression` (コンテキスト圧縮戦略)**:
    *   まず基本的なベクトル検索で多数のドキュメントチャンクを取得します。
    *   その後、LLM（または別の軽量なモデル）を使って、取得した各チャンクの中からユーザーの質問に直接関連する部分のみを抽出（圧縮）します。
    *   圧縮されたコンテキストのみを最終的なLLMに渡すことで、ノイズを減らし、より焦点を絞った効率的な回答生成を目指します。
*   **`deep_rag` (Deep RAG戦略)**:
    *   **目的**: 複雑なユーザーの質問に対し、より深い理解と網羅的な情報検索を行うことを目的とします。
    *   **処理フロー**:
        1.  **クエリ分解**: 受け取ったユーザーの質問を、LLM（`LLMManager`経由で取得）と専用のプロンプトテンプレート（`core/retriever_manager.py` 内で定義、将来的には設定ファイル管理）を用いて、複数のより単純なサブクエリに分解します。
        2.  **段階的検索**: 分解された各サブクエリについて、ベースとなるリトリーバー（現在は`BasicRetrieverStrategy`を使用）を用いて個別にドキュメント検索を実行します。
        3.  **結果統合**: 各サブクエリから得られたドキュメントリストを統合します。現状の実装では、単純なリスト結合と簡易的なIDベースの重複排除を行います。
        4.  **最終回答生成**: 統合されたドキュメント群をコンテキストとして、メインのLLMに最終的な回答を生成させます。
    *   **実装詳細**: `core/retriever_manager.py` 内の `DeepRagRetrieverStrategy` クラスおよびその内部カスタムリトリーバー `DeepRagCustomRetriever` によって実装されます。
    *   **パラメータ**: （将来的には `config/strategies.yaml` で設定可能）サブクエリの最大数、サブクエリごとの検索結果数など。
    *   **利点**: 複数ステップの思考や多様な側面からの情報収集が必要な質問に対して、より質の高い回答を生成できる可能性があります。
    *   **考慮事項**: LLMの呼び出し回数が増えるため、レイテンシが増加する可能性があります。クエリ分解の質が検索結果に大きく影響します。ドキュメント統合・再ランキング処理の高度化が今後の課題です。

これらの戦略は、LangChainライブラリの提供するリトリーバーやチェーンを活用して実装されます。ユーザーはフロントエンドのチャット画面でこれらの戦略をドロップダウンメニューから選択できます。

**将来的な拡張: ベクトルストア戦略のモジュール化**

現状のRAG戦略選択に加えて、将来的にはベクトルストアの構築方法（エンベディングモデルの選択、チャンキング手法の選択など）もモジュール化し、ユーザーが選択またはシステムが自動最適化できるようにすることを構想しています。これにより、ドキュメントの種類やタスクの性質に応じて、最適なドキュメント処理・検索戦略を柔軟に組み合わせることが可能になります。例えば、以下のような選択肢が考えられます。

*   **エンベディングモデル**: `all-MiniLM-L6-v2` (デフォルト), `text-embedding-ada-002` (OpenAI), その他の多言語対応モデルなど。
*   **チャンキング戦略**: 固定長チャンキング、再帰的チャンキング、意味的チャンキングなど。チャンクサイズやオーバーラップも調整可能とする。

このモジュール化により、MochiRAGはより多様なニーズに対応できる、柔軟で拡張性の高いRAGプラットフォームとなることを目指します。

### 4.1 ドキュメントインジェスト

* **データソース**: ユーザーはPDFやWord、テキスト、Markdownファイル、またはURLなどをアップロードまたは指定します。これらファイルはAzure Blob Storageなどに保管し、参照情報（ストレージURLやドキュメントID）をDBに記録します。
* **データソース選択・管理機能** : ユーザーは、複数のドキュメントを登録したデータソース群を管理できます。すなわち、RAGの検索対象とするデータソースを選択・管理できます。データソース毎にユーザーは名前や説明を付けることができ保存できます。
* **テキスト抽出・前処理**: PDFやスキャン画像の場合はOCRを行い、テキストデータを取得します。HTMLやMarkdownはタグ/構造を適宜除去し、テキストを正規化します。不要な改行や特殊記号は整理し、分析可能な状態にします。
* **チャンク分割**: ドキュメントをLLMの入力制限内に収めるため、段落や固定長（例: 1000ワード程度、設定ファイルで設定）で文書を分割します。このとき、ドキュメントタイトルやページ番号、節タイトルなどを各チャンクにメタデータとして付与し、後の引用表示に利用します。（将来的には選択されたベクトルストア戦略に基づいて動的に変更）
* **メタデータ付与**: 各チャンクにはドキュメントID、ページ番号、ユーザーIDなどを紐づけます。これにより、ベクトル検索時に「ユーザーID=ログインユーザー」の絞り込みを行い、他ユーザーのチャンクが検索結果に含まれないようにします。

### 4.2 埋め込み生成およびインデックス構築

* **埋め込みモデル選定**: （将来的には選択されたベクトルストア戦略に基づいて）チャンクテキストから意味的なベクトルを得るため、Sentence Transformers (`all-MiniLM-L6-v2`) やAzure OpenAIの`text-embedding-ada-002`などを利用します。モデル入力のトークン上限を超えないよう、チャンクサイズを調整します。
* **インデックス作成**: チャンクのベクトルとメタデータはChromaDBやAzure AI Searchなどのベクトル検索エンジンに格納します。Azure Cognitive Searchではインデクサーとスキルセットによる自動チャンク分割とベクトル化が可能です。登録時にユーザーIDでインデクシングし、検索時にフィルタリングできるよう設計します。別案として、Azure Cosmos DB (MongoDB API) やMilvus、Qdrantなどのベクトルデータストアも検討できます。
* **定期更新**: ドキュメントの追加・更新時には、差分を検出してベクトルDBを更新します。Azure Functionsなどでイベント駆動型に実装すると効率的です。またAzure AI Searchのインデクサーでスケジュール更新することも可能です。

### 4.3 検索 (ベクトル検索)

* **クエリベクトル化**: ユーザーの質問文を（将来的には選択されたベクトルストア戦略に基づいて）同じ埋め込みモデルでベクトル化します。
* **検索実行**: 選択されたRAG戦略に応じて、ベクトルDBに対し、「ユーザーID=現在ユーザー」でフィルタをかけ、コサイン類似度や内積で上位K件のチャンクを取得します。Kは初期値5～10程度とし、応答品質に応じて調整します。
* **再ランキング/圧縮**: `multi_query`や`contextual_compression`戦略では、追加の処理（複数クエリ結果のマージ、LLMによる関連箇所の抽出など）が行われます。
* **コンテキスト抽出**: 検索で得られたチャンクのテキストとそのメタデータ（ドキュメント名・ページ番号など）を取り出します。これらをLLMへのコンテキストとしてプロンプトに含めます。
* **オーケストレーター**: フロントエンドからのクエリ（選択されたRAG戦略を含む）と認証トークンをバックエンドが受け取り、適切な検索処理とLLM呼び出しを制御します。ユーザーIDに応じてベクトル検索で取得範囲を限定し、取得したチャンクをプロンプトに組み込んでLLMに渡します。

### 4.4 回答生成（LLMへのプロンプト）

* **プロンプト作成**: 検索結果のチャンク内容をプロンプトに整形し、ユーザー質問と合わせてLLMへ送信します。プロンプト例:

  ```
  【参照ドキュメント】
  ・DocA (p.10-12): <チャンク1のテキスト>
  ・DocB (p.3-4): <チャンク2のテキスト>
  ...
  【質問】: ここにユーザーからの質問を記述
  ```
* **回答指示**: システムメッセージで「取得したドキュメントのみを根拠に回答し、各主張に必ず出典を付ける」よう指示します。例えば「検索された文書からのみ回答し、引用付きで答えてください」とプロンプトします。取得できた情報で回答できない場合は「関連情報が見つかりませんでした」と明示させるように指示します。
* **モデル呼び出し**: Azure OpenAIのGPT-4やGPT-35-TurboなどのチャットコンプリーションAPIを利用します。Azure AI Foundryの「On Your Data」機能を使えば、ドキュメント検索と回答生成が統合されます。必要な場合はバッチ処理やキャッシュでレイテンシを最適化します。
* **ユーザーカスタマイズプロンプト**: 利用するプロンプトをユーザーが設定管理できます。

### 4.5 引用付き出力

* **参照番号付与**: LLMの回答には「\[1]」「\[2]」などの参照番号を含めます。これら番号は検索結果のチャンクに対応させます。参照番号は関連度順や取得順で振り、各番号が示すドキュメント名・ページを後述リストで示します。Azure OpenAIではフィールドマッピングを設定すると出力にファイル名が使えます。
* **出典リスト表示**: 回答下部に番号付きで出典情報を表示します。例: `[1] DocA (p.10-12)、[2] DocB (p.3-4)`のように記述します。UIでは各出典をクリックすると対応ドキュメントの該当箇所にジャンプするビューア機能を実装するとNotebookLM風になります。
* **回答検証**: 質問文がドキュメントで回答できない場合、回答文中で「（申し訳ありませんが情報が不足しています）」と述べさせます。また必要に応じて、システムで簡易チェック（例: 得られたコンテキストと質問の語彙一致率）を行い、不足時は追加入力を促すことも検討します。

## 参考資料

* Microsoft Learn: Quickstart – Add app authentication to your web app (Azure App Service)
* Microsoft Learn: Integrate applications with Microsoft Entra ID (アプリ認証/OAuth)
* LlamaIndex Blog: Building Multi-Tenancy RAG System
* Milvus Blog: Designing Multi-Tenancy RAG (Database/Collection/Partition)
* Microsoft Learn: Retrieval Augmented Generation (RAG) in Azure AI Search (概要)
* Microsoft Learn: Design a secure multitenant RAG solution (アーキテクチャ)
* Microsoft Learn: Azure OpenAI On Your Data (概要とプロンプト例)
* Microsoft Learn: Azure AI Search – Chunk large documents for vector search
* AWS: What is RAG? – Retrieval-Augmented Generation (概要と利点)
* LangChain Documentation on Retrievers: (例: [https://python.langchain.com/docs/modules/data_connection/retrievers/](https://python.langchain.com/docs/modules/data_connection/retrievers/))


FastAPI＋Pythonをバックエンドに、Streamlitや軽量なUIフレームワークを使ったフロントエンド、ベクトルDBにはOSS（Milvus/Qdrant/Chroma）を使用し、LLMはOllamaによるローカル動作を前提とした構成で、MochiRAGを実装する際の最適なフレームワークおよびツール構成を提案します。内容がまとまり次第お知らせします。


# MochiRAG システム構成案

MochiRAGは、FastAPIベースのバックエンドとシンプルなPython前端（例：Streamlit）を組み合わせたRAG（Retrieval-Augmented Generation）型QAシステムとして設計します。全文検索やナレッジベースから関連情報を取得し、大型言語モデル(LLM)で回答を生成する構成です。以下では各要素（バックエンド・フロントエンド・RAGフレームワーク・ベクトルDB・LLM・認証など）ごとに、推奨技術とその理由を示します。

*図: RAGシステム例のアーキテクチャ例（Streamlit front-end＋FastAPI back-end＋ベクトルDB＋LLM）。MochiRAGも同様に構築する。*

* **バックエンド（API層）:** Python/FastAPIを採用します。FastAPIは非同期対応で高性能かつシンプルなAPI構築が可能で、OpenAPIドキュメントも自動生成され保守性に優れます。FastAPIチュートリアルでもJWTによるOAuth2認証がサンプルとして示されており、ユーザー管理の実装も容易です。Dockerコンテナ化しておけば、ローカル開発から本番環境まで同一の実行環境を再現でき、コンテナオーケストレーションによる拡張も可能です。

* **フロントエンド（UI）:** PythonベースのStreamlitやGradioなどを利用します。StreamlitはインタラクティブUIをPythonコードだけで高速に実装でき、RAGシステム向けチャットインターフェイスに適しています。シンプルな操作性でメンテナンス負荷も低く、FastAPIと組み合わせればクライアント・サーバー分離型の構成が取れます。将来的にReactなど別のUI技術へ移行する場合も、REST APIベースなら容易に差し替え可能です。

* **RAGオーケストレーション:** LangChainやLlamaIndex（GPT Index）等のライブラリを利用し、以下の機能を提供します：ドキュメントのテキスト分割、ベクトル埋め込み、類似度検索、LLM呼び出しと回答生成のチェーン構築など。特にLangChainは多数のベクトルストア・LLMとの統合実装が用意されており、ローカルのOllamaモデルや将来のAzure OpenAIを切り替えやすい設計です（LangChainにはOllama用クラス`OllamaLLM`があり、Azure Cognitive Search用の`AzureSearch`ベクトルストアも備わっています）。テキスト分割にはLangChainの`TextSplitter`等を用い、HuggingFaceのSentence-Transformer系モデルやOpenAI埋め込みを活用します（例えば軽量モデルの`all-MiniLM-L6-v2`等）。これにより、検索クエリや文書チャンクを効率的にベクトル化・検索できます。 **MochiRAGでは、複数のRAG戦略（基本検索、Parent Document Retriever、Multi-Query Retriever、Contextual Compression Retriever、DeepRAGなど）を実装し、ユーザーが選択できるようにします。将来的には、ベクトルストア戦略（エンベディングモデル、チャンキング手法など）もモジュール化し、RAG戦略と組み合わせることで、より柔軟な検索・回答生成パイプラインの構築を目指します。**

* **ベクトルデータベース:** OSSベクトルDBとしてQdrantやMilvusを想定します。Milvusは「ビリオン規模のベクトル検索に対応した高性能・スケーラブルなベクトルDB」であり、RAG用途に適しています。一方Qdrantも「高性能な類似検索・動的シャーディングに優れた」ベクトルDBで、オープンソースかつスループットが高いことが報告されています。ChromaDBはPython組み込みで初期開発を素早く始められる利点があります。最終的にはAzure Cognitive Search（Azure AI Search）のベクトル検索に移行する想定のため、LangChainの`AzureSearch`統合等を活用し、別のストアへ抽象化を効かせられる実装にしておきます。

* **LLM（大規模言語モデル）:** ローカルではOllamaで稼働するLLM（例：Ollama上のLLaMA3系など）をAPI経由で利用します。LangChainの`OllamaLLM`クラスを使えば、ローカルのOllamaインスタンスへの呼び出しが簡単です。Ollama API（デフォルトはlocalhost:11434）が公開するエンドポイントにプロンプトを送ることで回答生成できます。本番移行時はAzure OpenAIやOpenAI APIに切り替える設計とし、LangChainの`AzureOpenAI`等でモデル変更できるよう環境変数等で管理します。Embeddingモデルも同様に、ローカルHuggingFaceモデルからクラウドの埋め込みAPIへ切替可能です。

* **ユーザー認証・管理:** Entra IDなどの重厚なID連携は不要なので、FastAPIのOAuth2/JWT機能でシンプルに実装します。FastAPI公式ドキュメントでもパスワード認証＋JWTトークンの例が示されており、パスワードハッシュ化（passlib）＋PyJWTで認証済トークンを発行すれば十分です。ユーザーデータはローカルDB（例：SQLiteやPostgreSQL）に格納し、FastAPIユーザー管理ライブラリ（fastapi-users等）を導入しても良いでしょう。

* **データ取り込み・管理:** ドキュメント（PDF/テキスト等）を受け取り、分割・ベクトル化してDBに登録するパイプラインを構築します。例えばLangChainのドキュメントローダーやHamiltonでバッチ処理し、文書のチャンクごとに埋め込みを生成してベクトルDBにインデックスするフローです。元データはファイルサーバ（ローカルFSやAzure Blobストレージ）に保存し、ベクトルDBにはメタデータ（文書IDやパス）を保持して検索時に元文書を参照できます。

* **デプロイとスケーラビリティ:** 全要素をDockerコンテナにパッケージ化し、開発環境から本番環境まで一貫性を持たせます。FastAPIとフロントエンドはそれぞれ独立したコンテナとし、docker-composeやKubernetesで管理します。将来的にAzure上でスケールさせる際は、Azure Kubernetes Serviceでコンテナ展開するか、ベクトルDBはAzure Cognitive Search、モデルはAzure OpenAIに差し替えます。

以上の構成により、MochiRAGはOSS技術とクラウド対応性を両立でき、拡張性や運用性も高いシステムになります。参考までに、Streamlit+FastAPI+Weaviate+OpenAI（Ollama）の例を挙げると、前端にStreamlit、バックエンドにFastAPI、ベクトルDBにWeaviate、LLMにOpenAI（埋め込み/生成）を利用したスケーラブルな構成が提案されています。これをMochiRAG仕様に置き換えれば、FastAPI/ LangChain、Qdrant/Milvus、Ollama/OpenAIといった選定の妥当性が確認できます。

**参考資料:** FastAPIチュートリアル、LangChainドキュメント、ベクトルDB比較 など。これらの技術選定により、要件を満たした柔軟かつ実用的なRAGシステムが実現します。
```
